mod node;

use node::TreeNode;
use num::{Bounded, Integer};
use std::marker::PhantomData;
use std::ops::Neg;

/// 2äººã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹æ‰‹ç•ªï¼
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Actor {
    /// æ€è€ƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼
    Agent,
    /// ãƒ¦ãƒ¼ã‚¶ãªã©ã®ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼
    Other,
}

/// ã‚²ãƒ¼ãƒ ã®çŠ¶æ…‹ï¼
pub trait State {
    /// ã“ã®çŠ¶æ…‹ãŒã™ã§ã«ã‚²ãƒ¼ãƒ çµ‚äº†æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ï¼
    fn is_game_over(&self) -> bool;
}

/// ã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹è¡Œå‹•ï¼
pub trait Action {
    /// ã“ã®è¡Œå‹•ã®æ‰‹ç•ªï¼
    fn actor(&self) -> Actor;
}

/// ã‚²ãƒ¼ãƒ å†…ã®çŠ¶æ…‹é·ç§»æ¡ä»¶ã‚’è¨˜è¿°ã™ã‚‹ï¼
pub trait Rule<S, A> {
    /// ã‚ã‚‹çŠ¶æ…‹ã«ãŠã„ã¦å®Ÿè¡Œå¯èƒ½ãªè¡Œå‹•ã‚’åˆ—æŒ™ã™ã‚‹éš›ã«ä½¿ç”¨ã™ã‚‹å‹ï¼
    type ActionIterator: IntoIterator<Item = A>;

    /// æŒ‡å®šã•ã‚ŒãŸçŠ¶æ…‹ä¸‹ã§å®Ÿè¡Œå¯èƒ½ãªè¡Œå‹•ã‚’åˆ—æŒ™ã™ã‚‹ï¼
    fn iterate_available_actions(&self, state: &S, actor: Actor) -> Self::ActionIterator;

    /// çŠ¶æ…‹ã‚’é·ç§»ã•ã›ã‚‹ï¼
    fn translate_state(&self, state: &S, action: &A) -> S;
}

/// ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã®è©•ä¾¡é–¢æ•°ï¼
pub trait Evaluator<S> {
    /// è©•ä¾¡æŒ‡æ¨™ã¨ãªã‚‹å‹ï¼
    type Evaluation: Copy + Ord + Bounded + Neg<Output = Self::Evaluation>;

    /// æŒ‡å®šã•ã‚ŒãŸçŠ¶æ…‹ã«ã¤ã„ã¦ï¼Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¨ã£ã¦ã®æœ‰åˆ©åº¦åˆã„ã‚’è©•ä¾¡ã™ã‚‹ï¼
    fn evaluate_for_agent(&self, state: &S) -> Self::Evaluation;
}

/// 2äººå®Œå…¨æƒ…å ±ã‚²ãƒ¼ãƒ ã®æ‰‹ã‚’ãƒã‚¬ã‚¢ãƒ«ãƒ•ã‚¡æ³•ã§æ€è€ƒã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼
pub struct NegaAlphaStrategy<'r, S, A, R, E> {
    /// ã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ«ï¼
    rule: &'r R,
    /// è©•ä¾¡é–¢æ•°ï¼
    evaluator: E,
    /// ğŸ‘»ğŸ‘»ğŸ‘»
    _s: PhantomData<S>,
    /// ğŸ‘»ğŸ‘»ğŸ‘»
    _a: PhantomData<A>,
}

/// ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹æ³•ã§åˆ©ç”¨ã™ã‚‹ã‚²ãƒ¼ãƒ æœ¨ã®ãƒãƒ¼ãƒ‰ï¼
#[derive(Debug)]
struct MinimaxNode<S, A, E> {
    /// ç¾åœ¨ã®çŠ¶æ…‹ï¼
    state: S,
    /// ã“ã®çŠ¶æ…‹ã«è‡³ã‚‹éš›ã«å®Ÿè¡Œã•ã‚ŒãŸè¡Œå‹•ï¼
    cause_action: A,
    /// ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¨ã£ã¦ã®ç¾åœ¨çŠ¶æ…‹ã®è©•ä¾¡å€¤ï¼
    evaluation: E,
}

impl Actor {
    pub fn opponent(&self) -> Self {
        match self {
            Actor::Agent => Actor::Other,
            Actor::Other => Actor::Agent,
        }
    }
}

impl<'r, S, A, R, E> NegaAlphaStrategy<'r, S, A, R, E>
where
    S: State,
    A: Action,
    R: Rule<S, A>,
    E: Evaluator<S>,
{
    /// æŒ‡å®šã—ãŸã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ«ãŠã‚ˆã³è©•ä¾¡é–¢æ•°ã®ã‚‚ã¨æ€è€ƒã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹ï¼
    pub fn new(rule: &'r R, evaluator: E) -> Self {
        Self {
            rule,
            evaluator,
            _s: PhantomData,
            _a: PhantomData,
        }
    }

    /// ãƒã‚¬ã‚¢ãƒ«ãƒ•ã‚¡æ³•ã«ã‚ˆã‚Šï¼Œç¾åœ¨ã®çŠ¶æ…‹ã«å¯¾ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ›ã¾ã—ã„è¡Œå‹•ã‚’æ¢ç´¢ã™ã‚‹ï¼
    /// # Params
    /// 1. state ç¾åœ¨ã®çŠ¶æ…‹
    /// 1. search_depth ä½•æ‰‹å…ˆã¾ã§èª­ã‚€ã‹ï¼ä¾‹ãˆã°ï¼Œæ¬¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ‰‹ã¾ã§ã®ã¿èª­ã‚€ãªã‚‰ï¼Œ`search_depth`ã¯1ã«ã™ã‚Œã°è‰¯ã„ï¼
    ///
    /// # Returns
    /// 1ç¨®é¡ä»¥ä¸Šã®è¡Œå‹•ãŒå¯èƒ½ãªå ´åˆï¼Œãã®ä¸­ã®æœ€ã‚‚æœ›ã¾ã—ã„è¡Œå‹•`action`ã‚’`Some(action)`ã¨ã—ã¦è¿”ã™ï¼
    ///
    /// å¯èƒ½ãªè¡Œå‹•ãŒãªã„å ´åˆï¼Œ`None`ã‚’è¿”ã™ï¼
    ///
    /// # Panics
    /// è©•ä¾¡å€¤`e`ã®æ­£è² åè»¢`-e`ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã—ãŸå ´åˆï¼
    pub fn search_action<N: Copy + Integer>(&self, state: &S, search_depth: N) -> Option<A> {
        self.rule
            // ç¾åœ¨ã®çŠ¶æ…‹ã«å¯¾ã—ã¦ï¼Œã“ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã§ãã‚‹è¡Œå‹•ã‚’ã¾ãšåˆ—æŒ™
            .iterate_available_actions(state, Actor::Agent)
            .into_iter()
            // è©¦ã—ã«è¡Œå‹•å¾Œã®ãƒãƒ¼ãƒ‰ã‚’ä½œã£ã¦ã¿ã‚‹
            .map(|agent_action| {
                let next_state = self.rule.translate_state(state, &agent_action);
                TreeNode::new(MinimaxNode::new(
                    next_state,
                    agent_action,
                    self.evaluator.evaluate_for_agent(state),
                ))
            })
            // ã•ã‚‰ã«ãã®å¾Œã®æ‰‹ã‚’ãƒã‚¬ã‚¢ãƒ«ãƒ•ã‚¡æ³•ã«ã‚ˆã‚Šèª­ã‚€ï¼
            // æœ€çµ‚çš„ã«ï¼Œãã®å¾Œã®æ‰‹ã®ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹è©•ä¾¡å€¤ãŒã“ã®ãƒãƒ¼ãƒ‰ã®è©•ä¾¡å€¤ã¨ãªã‚‹ï¼
            .map(|mut root| {
                let alpha = E::Evaluation::min_value();
                let beta = E::Evaluation::max_value();
                // ãƒã‚¬ã‚¢ãƒ«ãƒ•ã‚¡æ³•ã§ã¯ï¼Œæ‰‹ç•ªãŒå¤‰ã‚ã‚‹ãŸã³ã«è©•ä¾¡é–¢æ•°ã®ç¬¦å·ã‚’åè»¢ã•ã›ã‚‹ã“ã¨ã§ï¼Œè‡ªä»–ã®æ‰‹ã‚’çµ±åˆã—ã¦æ€è€ƒã™ã‚‹ï¼
                let root_evaluation =
                    -self.alpha_beta(search_depth - N::one(), &mut root, -beta, -alpha);
                (root, root_evaluation)
            })
            // ã‚‚ã£ã¨ã‚‚è©•ä¾¡å€¤ãŒè‰¯ã„è¡Œå‹•ã‚’é¸æŠã™ã‚‹ï¼
            .max_by(
                |(_left_root, left_evaluation), (_right_root, right_evaluation)| {
                    left_evaluation.cmp(right_evaluation)
                },
            )
            .map(|(root, _evaluation)| root.into_inner().cause_action)
    }

    fn alpha_beta<N: Copy + Integer>(
        &self,
        remaining_depth: N,
        current_node: &mut TreeNode<MinimaxNode<S, A, E::Evaluation>>,
        alpha: E::Evaluation,
        beta: E::Evaluation,
    ) -> E::Evaluation {
        use std::cmp::max;

        // æ³¨ç›®ãƒãƒ¼ãƒ‰ãŒæœ«ç«¯ãƒãƒ¼ãƒ‰ãªã‚‰ï¼Œæ³¨ç›®ãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ã«å¯¾ã™ã‚‹è©•ä¾¡å€¤ã‚’è¿”ã™ï¼
        if remaining_depth.is_zero() || current_node.item().state.is_game_over() {
            return current_node.item().evaluation;
        }
        // who WILL act on the current state?
        let next_actor = current_node.item().cause_action.actor().opponent();
        // æ¬¡ã®å®Ÿç¾ã—ã†ã‚‹çŠ¶æ…‹ã‚’ã™ã¹ã¦åˆ—æŒ™ã—ï¼Œãã‚Œã‚‰ã‚’ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ã®å­ã«åŠ ãˆã‚‹ï¼
        for action in self
            .rule
            .iterate_available_actions(&current_node.item().state, next_actor)
        {
            let minimax_node = {
                let next_state = self
                    .rule
                    .translate_state(&current_node.item().state, &action);
                // ãƒã‚¬ã‚¢ãƒ«ãƒ•ã‚¡æ³•ã§ã¯ï¼Œæ‰‹ç•ªã«ã‚ˆã£ã¦è©•ä¾¡å€¤ã®æ­£è² ã‚’åè»¢ã•ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼
                let evaluation = match next_actor {
                    Actor::Agent => -self.evaluator.evaluate_for_agent(&next_state),
                    Actor::Other => self.evaluator.evaluate_for_agent(&next_state),
                };
                MinimaxNode::new(next_state, action, evaluation)
            };
            current_node.add_child(minimax_node);
        }

        // å­ãƒãƒ¼ãƒ‰ã®è©•ä¾¡å€¤ã‚’å†å¸°çš„ã«æ±‚ã‚ã‚‹ï¼
        let mut alpha = alpha;
        for child in current_node.children_mut() {
            let next_depth = remaining_depth - N::one();
            // ãƒã‚¬ã‚¢ãƒ«ãƒ•ã‚¡æ³•ã§ã¯ï¼Œæ‰‹ç•ªãŒå¤‰ã‚ã‚‹ãŸã³ã«è©•ä¾¡é–¢æ•°ã®ç¬¦å·ã‚’åè»¢ã•ã›ã‚‹ã“ã¨ã§ï¼Œè‡ªä»–ã®æ‰‹ã‚’çµ±åˆã—ã¦æ€è€ƒã™ã‚‹ï¼
            alpha = max(alpha, -self.alpha_beta(next_depth, child, -beta, -alpha));
            // Î±ã‚«ãƒƒãƒˆ
            if alpha >= beta {
                break;
            }
        }
        // å­ãƒãƒ¼ãƒ‰ãŸã¡ã®æœ€çµ‚çš„ãªè©•ä¾¡å€¤ã‚’ã“ã®ãƒãƒ¼ãƒ‰ã«åæ˜ 
        current_node.item_mut().evaluation = alpha;
        //
        alpha
    }
}

impl<S, A, E> MinimaxNode<S, A, E> {
    fn new(state: S, cause_action: A, evaluation: E) -> Self {
        Self {
            state,
            cause_action,
            evaluation,
        }
    }
}

#[cfg(test)]
mod test_cmp {
    #[test]
    fn test_cmp() {
        let v = vec![5, 2, 0, 9, 4, 3];
        let max = v.into_iter().max_by(|left, right| left.cmp(right));
        assert_eq!(Some(9), max);
    }
}
