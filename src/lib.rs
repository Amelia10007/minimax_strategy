mod node;

use data_structure::Range;
use node::TreeNode;
use num::{Bounded, Integer};
use std::marker::PhantomData;

/// 2äººã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹æ‰‹ç•ªï¼
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum Actor {
    /// æ€è€ƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼
    Agent,
    /// ãƒ¦ãƒ¼ã‚¶ãªã©ã®ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼
    Other,
}

/// ã‚²ãƒ¼ãƒ ã®çŠ¶æ…‹ï¼
pub trait State {
    /// ã“ã®çŠ¶æ…‹ãŒã™ã§ã«ã‚²ãƒ¼ãƒ çµ‚äº†æ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã‚‹ã‹ï¼
    fn is_game_over(&self) -> bool;
}

/// ã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹è¡Œå‹•ï¼
pub trait Action {
    /// ã“ã®è¡Œå‹•ã®æ‰‹ç•ªï¼
    fn actor(&self) -> Actor;
}

/// ã‚²ãƒ¼ãƒ å†…ã®çŠ¶æ…‹é·ç§»æ¡ä»¶ã‚’è¨˜è¿°ã™ã‚‹ï¼
pub trait Rule<S, A> {
    /// ã‚ã‚‹çŠ¶æ…‹ã«ãŠã„ã¦å®Ÿè¡Œå¯èƒ½ãªè¡Œå‹•ã‚’åˆ—æŒ™ã™ã‚‹éš›ã«ä½¿ç”¨ã™ã‚‹å‹ï¼
    type ActionIterator: IntoIterator<Item = A>;

    /// æŒ‡å®šã•ã‚ŒãŸçŠ¶æ…‹ä¸‹ã§å®Ÿè¡Œå¯èƒ½ãªè¡Œå‹•ã‚’åˆ—æŒ™ã™ã‚‹ï¼
    fn iterate_available_actions(state: &S, actor: Actor) -> Self::ActionIterator;

    /// çŠ¶æ…‹ã‚’é·ç§»ã•ã›ã‚‹ï¼
    fn translate_state(state: &S, action: &A) -> S;
}

/// ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã®è©•ä¾¡é–¢æ•°ï¼
pub trait Evaluator<S> {
    /// è©•ä¾¡æŒ‡æ¨™ã¨ãªã‚‹å‹ï¼
    type Evaluation;

    /// æŒ‡å®šã•ã‚ŒãŸçŠ¶æ…‹ã«ã¤ã„ã¦ï¼Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¨ã£ã¦ã®æœ‰åˆ©åº¦åˆã„ã‚’è©•ä¾¡ã™ã‚‹ï¼
    fn evaluate_for_agent(state: &S) -> Self::Evaluation;
}

/// 2äººé›¶å’Œã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹é©åˆ‡ãªè¡Œå‹•ã‚’Î±Î²æ³•ã§æ€è€ƒã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼
pub struct AlphaBetaStrategy<S, A, R, E> {
    /// ğŸ‘»
    _s: PhantomData<S>,
    /// ğŸ‘»
    _a: PhantomData<A>,
    /// ğŸ‘»
    _r: PhantomData<R>,
    /// ğŸ‘»
    _e: PhantomData<E>,
}

/// ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹æ³•ã§åˆ©ç”¨ã™ã‚‹ã‚²ãƒ¼ãƒ æœ¨ã®ãƒãƒ¼ãƒ‰ï¼
#[derive(Debug)]
struct MinimaxNode<S, A, E> {
    /// ç¾åœ¨ã®çŠ¶æ…‹ï¼
    state: S,
    /// ã“ã®çŠ¶æ…‹ã«è‡³ã‚‹éš›ã«å®Ÿè¡Œã•ã‚ŒãŸè¡Œå‹•ï¼
    cause_action: Option<A>,
    /// ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¨ã£ã¦ã®ç¾åœ¨çŠ¶æ…‹ã®è©•ä¾¡å€¤ï¼
    evaluation: Option<E>,
}

impl Actor {
    /// ã“ã®æ‰‹ç•ªã«å¯¾ã™ã‚‹ç›¸æ‰‹å´ã®æ‰‹ç•ªã‚’è¿”ã™ï¼
    pub fn opponent(&self) -> Self {
        match self {
            Actor::Agent => Actor::Other,
            Actor::Other => Actor::Agent,
        }
    }
}

impl<S, A, R, E> AlphaBetaStrategy<S, A, R, E>
where
    S: State,
    A: Action,
    R: Rule<S, A>,
    E: Evaluator<S>,
    E::Evaluation: Copy + Ord + Bounded,
{
    /// æŒ‡å®šã—ãŸã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ«ãŠã‚ˆã³è©•ä¾¡é–¢æ•°ã®ã‚‚ã¨æ€è€ƒã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹ï¼
    pub fn new() -> Self {
        Self {
            _s: PhantomData,
            _a: PhantomData,
            _r: PhantomData,
            _e: PhantomData,
        }
    }

    /// Î±Î²æ³•ã«ã‚ˆã‚Šï¼Œç¾åœ¨ã®çŠ¶æ…‹ã«å¯¾ã™ã‚‹ã“ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ›ã¾ã—ã„è¡Œå‹•ã‚’æ¢ç´¢ã™ã‚‹ï¼
    /// # Params
    /// 1. state ç¾åœ¨ã®çŠ¶æ…‹
    /// 1. search_depth ä½•æ‰‹å…ˆã¾ã§èª­ã‚€ã‹ï¼ä¾‹ãˆã°ï¼Œæ¬¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ‰‹ã¾ã§ã®ã¿èª­ã‚€ãªã‚‰ï¼Œ`search_depth`ã¯1ã«ã™ã‚Œã°è‰¯ã„ï¼
    ///
    /// # Returns
    /// 1ç¨®é¡ä»¥ä¸Šã®è¡Œå‹•ãŒå¯èƒ½ãªå ´åˆï¼Œãã®ä¸­ã®æœ€ã‚‚æœ›ã¾ã—ã„è¡Œå‹•`a`ã¨ãã®è¡Œå‹•å¾Œã®çŠ¶æ…‹`s`ã‚’`Some((a, s))`ã¨ã—ã¦è¿”ã™ï¼
    ///
    /// å¯èƒ½ãªè¡Œå‹•ãŒãªã„å ´åˆï¼Œ`None`ã‚’è¿”ã™ï¼
    pub fn search_action<N: Copy + Integer>(&self, state: S, search_depth: N) -> Option<(A, S)> {
        let mut root = TreeNode::new(MinimaxNode::new(state, None, None));
        self.construct_best_game_tree_alpha_beta(
            search_depth,
            &mut root,
            Range::new(E::Evaluation::min_value(), E::Evaluation::max_value()),
        )
        .and_then(|_| {
            root.into_child().and_then(|best_node| {
                let inner = best_node.into_inner();
                let next_state = inner.state;
                inner.cause_action.map(|action| (action, next_state))
            })
        })
    }

    /// Î±Î²æ³•ã«ã‚ˆã‚Šï¼ŒæŒ‡å®šã—ãŸãƒãƒ¼ãƒ‰ã®è©•ä¾¡å€¤ã‚’å†å¸°çš„ã«è¨ˆç®—ã™ã‚‹ï¼
    /// # Params
    /// 1. remaining_depth æ®‹ã‚Šã®æ¢ç´¢æ·±ã•ï¼
    /// 1. current_node æ³¨ç›®ãƒãƒ¼ãƒ‰ï¼
    /// 1. alpha è©•ä¾¡å€¤ã®é–¢å¿ƒç¯„å›²ã®ä¸‹é™ï¼
    /// 1. beta è©•ä¾¡å€¤ã®é–¢å¿ƒç¯„å›²ã®ä¸Šé™ï¼
    ///
    /// # Returns
    /// `Some(e)`: ã“ã®ãƒãƒ¼ãƒ‰ã®è©•ä¾¡å€¤`e`
    ///
    /// `None`: ã“ã®ãƒãƒ¼ãƒ‰ãŒã‚²ãƒ¼ãƒ çµ‚äº†ãƒãƒ¼ãƒ‰ã§ã¯ãªãï¼Œã‹ã¤å–ã‚Œã‚‹è¡Œå‹•ãŒãªã„å ´åˆ
    fn construct_best_game_tree_alpha_beta<N: Copy + Integer>(
        &self,
        remaining_depth: N,
        current_node: &mut TreeNode<MinimaxNode<S, A, E::Evaluation>>,
        evaluation_range: Range<E::Evaluation>,
    ) -> Option<E::Evaluation> {
        // ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ (æ¶ˆã—ã¦ã‚‚å•é¡Œãªã„ã‘ã©ï¼Œã‚³ãƒ¼ãƒ‰å¤‰æ›´ã—ãŸéš›ã®æŒ™å‹•æ¤œè¨¼ã®ãŸã‚ã«ä¸€å¿œã¨ã£ã¦ãŠã)
        debug_assert!(current_node.evaluation.is_none());

        // æ³¨ç›®ãƒãƒ¼ãƒ‰ãŒæœ«ç«¯ãƒãƒ¼ãƒ‰ãªã‚‰ï¼Œç¾åœ¨ã®çŠ¶æ…‹ã«å¯¾ã™ã‚‹é™çš„è©•ä¾¡å€¤ã‚’ãã®ã¾ã¾é©ç”¨ã™ã‚‹
        if remaining_depth.is_zero() || current_node.state.is_game_over() {
            let evaluation = E::evaluate_for_agent(&current_node.state);
            current_node.evaluation = Some(evaluation);
            return Some(evaluation);
        }

        // who WILL act on the current state?
        let next_actor = match current_node.cause_action.as_ref() {
            Some(action) => action.actor().opponent(),
            None => Actor::Agent,
        };

        // çŠ¶æ…‹é·ç§»ãªã©ã«ä½¿ç”¨ã™ã‚‹ã®ã§ï¼Œæ³¨ç›®ãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‚’ã¨ã£ã¦ãŠãï¼
        // ã“ã“ã§ã¯æ§‹é€ ä½“ã®ï¼Œå¾Œã®å‡¦ç†ã§å¤‰æ›´ã•ã‚Œãªã„ãƒ¡ãƒ³ãƒã ã‘ã®å‚ç…§ã‚’ä¿æŒã™ã‚‹ã ã‘ãªã®ã§ï¼Œ
        // unsafeãƒ–ãƒ­ãƒƒã‚¯ã®å‡¦ç†ã¯å®‰å…¨ã§ã‚ã‚‹ï¼
        let current_state = {
            let pointer: *const _ = &current_node.state;
            unsafe { pointer.as_ref().unwrap() }
        };
        let mut current_evaluation_range = evaluation_range;

        // æ¬¡ã®å®Ÿç¾ã—ã†ã‚‹çŠ¶æ…‹ã‚’ã™ã¹ã¦åˆ—æŒ™ã—ï¼Œã²ã¨ã¤ã²ã¨ã¤èª¿ã¹ã‚‹
        for mut child in R::iterate_available_actions(&current_state, next_actor)
            .into_iter()
            .map(|action| {
                let next_state = R::translate_state(&current_state, &action);
                MinimaxNode::new(next_state, Some(action), None)
            })
            .map(|minimax_node| TreeNode::new(minimax_node))
        {
            // å­ãƒãƒ¼ãƒ‰ã®è©•ä¾¡å€¤ã‚’å†å¸°çš„ã«æ±‚ã‚ã‚‹ï¼
            // ã“ã“ã§NoneãŒå¸°ã£ã¦ããŸå ´åˆï¼Œãã®å­ãƒãƒ¼ãƒ‰ã¯ã‚²ãƒ¼ãƒ çµ‚äº†ã§ã‚‚ãªãï¼Œã‹ã¤å–ã‚Œã‚‹è¡Œå‹•ãŒãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã®ã§ï¼Œæ¢ç´¢å¯¾è±¡ã¨ã—ãªã„ï¼
            let child_evaluation = match self.construct_best_game_tree_alpha_beta(
                remaining_depth - N::one(),
                &mut child,
                current_evaluation_range,
            ) {
                Some(e) => e,
                None => continue,
            };
            // ãƒŸãƒ‹ãƒãƒƒã‚¯ã‚¹æ³•ã«ã‚ˆã‚Šï¼Œæ¢ç´¢ã™ã‚‹å¿…è¦ãŒã‚ã‚‹æã ã‘ã‚’é¸æŠã™ã‚‹
            if let Some(e) = current_node.evaluation {
                match next_actor {
                    // ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è‡ªåˆ†ãŒæœ‰åˆ©ã«ãªã‚‹è¡Œå‹•ã‚’é¸æŠã™ã‚‹ã®ã§ï¼Œ
                    // è‡ªåˆ†ãŒä¸åˆ©ã«ãªã‚‹è¡Œå‹•ã¯å€™è£œã‹ã‚‰é™¤å¤–ã™ã‚‹
                    Actor::Agent => {
                        if e >= child_evaluation {
                            continue;
                        }
                    }
                    // ç›¸æ‰‹ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä¸åˆ©ã«ãªã‚‹è¡Œå‹•ã‚’é¸æŠã™ã‚‹ã®ã§ï¼Œ
                    // ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæœ‰åˆ©ã«ãªã‚‹è¡Œå‹•ã¯å€™è£œã‹ã‚‰é™¤å¤–ã™ã‚‹ï¼
                    Actor::Other => {
                        if e <= child_evaluation {
                            continue;
                        }
                    }
                }
            }
            // ã“ã“ã«æ¥ãŸã¨ã„ã†ã“ã¨ã¯ï¼Œã‚ˆã‚Šè‰¯ã„å­ãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã£ãŸã¨ã„ã†ã“ã¨ãªã®ã§ï¼Œå­ãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’å…¥ã‚Œæ›¿ãˆã‚‹ï¼
            // ã¾ãŸï¼Œæ³¨ç›®ãƒãƒ¼ãƒ‰ã®è©•ä¾¡å€¤ã«ã¯ï¼Œå­ãƒãƒ¼ãƒ‰ã®å€¤ã‚’åæ˜ ã•ã›ã‚‹ï¼
            current_node.replace_child(child);
            current_node.evaluation = Some(child_evaluation);
            // è©•ä¾¡å€¤ã®æ³¨ç›®ç¯„å›²ã‚’æ›´æ–°ã™ã‚‹ï¼
            // å¯èƒ½ãªã‚‰ï¼ŒÎ±Î²ã‚«ãƒƒãƒˆã—ã¦æ¢ç´¢é‡ã‚’æ¸›ã‚‰ã™ï¼
            let maybe_next_range = match next_actor {
                Actor::Agent => Range::try_new(child_evaluation, current_evaluation_range.max),
                Actor::Other => Range::try_new(current_evaluation_range.min, child_evaluation),
            };
            match maybe_next_range {
                Some(range) => current_evaluation_range = range,
                None => break,
            }
        }

        // æ³¨ç›®ãƒãƒ¼ãƒ‰ã®æœ€çµ‚çš„ãªè©•ä¾¡å€¤ã‚’è¿”ã™ï¼
        // ã“ã“ã«åˆ°é”ã—ãŸæ™‚ç‚¹ã§è©•ä¾¡å€¤ãŒæ±ºå®šã—ã¦ã„ãªã„ã¨ã„ã†ã“ã¨ã¯ï¼Œ
        // æ³¨ç›®ãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‹ã‚‰å–ã‚Œã‚‹è¡Œå‹•ãŒãªã„ã¨ã„ã†ã“ã¨ãªã®ã§ï¼Œ
        // ãã®ã‚ˆã†ãªãƒãƒ¼ãƒ‰ã¯æ¢ç´¢ã®å¯¾è±¡ã«ã—ãªã„ï¼
        current_node.evaluation
    }
}

impl<S, A, E> MinimaxNode<S, A, E> {
    fn new(state: S, cause_action: Option<A>, evaluation: Option<E>) -> Self {
        Self {
            state,
            cause_action,
            evaluation,
        }
    }
}

#[cfg(test)]
mod test_cmp {
    #[test]
    fn test_cmp() {
        let v = vec![5, 2, 0, 9, 4, 3];
        let max = v.into_iter().max_by(|left, right| left.cmp(right));
        assert_eq!(Some(9), max);
    }
}
